{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings Experiment 03 - Implementation of Embedding Averaging (Initialization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E4sYsciBpX5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2iprj4pBrUb"
      },
      "outputs": [],
      "source": [
        "# Load the GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb-cm5SNJCB7"
      },
      "outputs": [],
      "source": [
        "new_tokens = ['<FUNC1_STT>', '<FUNC2_STT>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3BPObKFBsy8",
        "outputId": "e022282f-49d9-4687-eedc-f82023bc5fdc"
      },
      "outputs": [],
      "source": [
        "# Add the new words to the tokenizer's vocabulary\n",
        "\n",
        "\n",
        "tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "# Resize the GPT-2 model's embedding layer to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdAxy--uWjCc"
      },
      "outputs": [],
      "source": [
        "params = model.state_dict()\n",
        "embeddings = params['transformer.wte.weight']\n",
        "pre_expansion_embeddings = embeddings[:-2,:]\n",
        "mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
        "n = pre_expansion_embeddings.size()[0]\n",
        "sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
        "dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
        "        mu, covariance_matrix=1e-5*sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUzs8zZMWoRo",
        "outputId": "bfd0b23b-e1c1-4a3e-860b-6fe0480a263f"
      },
      "outputs": [],
      "source": [
        "new_embeddings = torch.stack(tuple((dist.sample() for _ in range(3))), dim=0)\n",
        "embeddings[-3:,:] = new_embeddings\n",
        "params['transformer.wte.weight'][-3:,:] = new_embeddings\n",
        "model.load_state_dict(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "9RBxVcQMWqbZ",
        "outputId": "af4a2657-8cfa-4c1c-a6fa-e74ad3e11424"
      },
      "outputs": [],
      "source": [
        "sent2 = 'Dogs are great because they are '\n",
        "tokenizer.decode(model.generate(**tokenizer(sent2, return_tensors='pt'), do_sample=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmWiTh_4Bujy",
        "outputId": "b436ee80-680b-42c9-d00d-19471de413eb"
      },
      "outputs": [],
      "source": [
        "# Print embeddings before training\n",
        "print(\"Embeddings before training:\")\n",
        "new_token_embeddings = model.transformer.wte.weight[-2:]  # Get embeddings for the new tokens\n",
        "print(new_token_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRvpkXiNW9ii",
        "outputId": "473f041b-26ce-4b4c-bc36-9ec790a72438"
      },
      "outputs": [],
      "source": [
        "embeddings_np = new_token_embeddings.detach().cpu().numpy()\n",
        "\n",
        "# Get corresponding words for the embeddings\n",
        "words = tokenizer.convert_ids_to_tokens(range(len(tokenizer)), skip_special_tokens=True)\n",
        "\n",
        "# Print words corresponding to the embeddings\n",
        "for i, embedding in enumerate(embeddings_np):\n",
        "    word = words[-2 + i]  # Get the word corresponding to the embedding\n",
        "    print(f\"Embedding {i+1}: Word: {word}, Embedding: {embedding[0]}, {embedding[1]}, ... {embedding[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjw8ZDVGBwYC"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "dataset_path = 'usage_dataset_2.txt'\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "    return tokens\n",
        "\n",
        "def process_dataset(dataset):\n",
        "    tokenized_dataset = []\n",
        "    sentences = dataset.split('\\n')\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip() != '':\n",
        "            tokens = tokenize_sentence(sentence)\n",
        "            tokenized_dataset.append(tokens)\n",
        "    return tokenized_dataset\n",
        "\n",
        "# Read the dataset file\n",
        "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
        "    dataset = file.read()\n",
        "\n",
        "# Tokenize each sentence individually\n",
        "tokenized_dataset = process_dataset(dataset)\n",
        "\n",
        "# Pad the tokenized sequences\n",
        "padded_dataset = pad_sequence([torch.tensor(tokens) for tokens in tokenized_dataset], batch_first=True)\n",
        "\n",
        "# Convert tokenized dataset to PyTorch tensors\n",
        "inputs = padded_dataset[:, :-1]  # Exclude the last token for prediction\n",
        "labels = padded_dataset[:, 1:]   # Shift the input to generate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKyzrLLMB0Cs",
        "outputId": "33ad3aef-5173-4ff0-8292-a632b00c559f"
      },
      "outputs": [],
      "source": [
        "# Convert tokenized dataset to PyTorch tensors\n",
        "inputs = torch.tensor(padded_dataset[:-1]).unsqueeze(0)  # Exclude the last token for prediction\n",
        "labels = torch.tensor(padded_dataset[1:]).unsqueeze(0)   # Shift the input to generate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIMxS4WnZJ1L",
        "outputId": "34710381-203b-4715-a863-371d61a03359"
      },
      "outputs": [],
      "source": [
        "print(inputs[0])\n",
        "print(inputs.shape)\n",
        "print(labels[0])\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "puuB6JTMZvRZ",
        "outputId": "29bf689b-bf45-4ad1-fb14-81c89e85fbd3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(type(inputs.squeeze().detach().numpy()))\n",
        "\n",
        "input_tokens = tokenizer.decode(inputs.squeeze().numpy(), skip_special_tokens=True)\n",
        "label_tokens = tokenizer.decode(labels.squeeze(), skip_special_tokens=True)\n",
        "\n",
        "print(\"Input tokens:\", input_tokens)\n",
        "print(\"Label tokens:\", label_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YraaVLwTF2VO",
        "outputId": "ab278f18-8228-494e-d24c-c6d4c6706175"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg1WRwMoB3mn",
        "outputId": "55c205de-3334-4074-92ae-2468bd439825"
      },
      "outputs": [],
      "source": [
        "# Set the model in training mode\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# Fine-tune the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "for epoch in range(40):  # You can adjust the number of epochs as needed\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MB8Z8W0B5TS",
        "outputId": "11a85409-4d7a-4de8-b4e2-4a8e28458468"
      },
      "outputs": [],
      "source": [
        "# Print embeddings after training\n",
        "print(\"\\nEmbeddings after training:\")\n",
        "new_token_embeddings = model.transformer.wte.weight[-2:]  # Get embeddings for the new tokens\n",
        "print(new_token_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGGWjz7tB7T0"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "save_path = 'fine_tuned_model.pth'\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35bRLlGqB80a",
        "outputId": "0623e8da-09b8-4230-8e4f-1594df7d8de0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate cosine similarity between each pair of new embeddings\n",
        "similarity_matrix = cosine_similarity(new_token_embeddings.cpu().detach().numpy())\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(\"\\nCosine Similarity Matrix:\")\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNPPIhJbG6vD",
        "outputId": "6ce06bc1-b086-45bb-df68-ceabdf73ecfb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate cosine similarity between each pair of new embeddings\n",
        "similarity_matrix = cosine_similarity(new_token_embeddings.cpu().detach().numpy())\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(\"\\nCosine Similarity Matrix:\")\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvOdud7VbPuS",
        "outputId": "894aa69c-e19a-4628-8018-8b643f3a7aea"
      },
      "outputs": [],
      "source": [
        "embedding_layer = model.transformer.wte\n",
        "\n",
        "# Specify the character for which you want to retrieve the embedding\n",
        "character = \"!\"\n",
        "\n",
        "# Convert the character to its corresponding token ID using the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "token_id = tokenizer.encode(character, add_special_tokens=False)[0]\n",
        "\n",
        "# Retrieve the embedding vector for the token ID\n",
        "embedding = embedding_layer.weight[token_id]\n",
        "\n",
        "# Print the embedding vector\n",
        "print(\"Embedding for character '{}':\".format(character))\n",
        "print(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouTTF8qbHbEW"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, max_length=50, temperature=0.8):\n",
        "    input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Decode and return generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV3_hKZLHcO9",
        "outputId": "a87f636b-b1ba-4731-8726-bcd3f0c0b1f9"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(\"I looked through the thesaurus and found that the synonym for pretty is\")\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhq5ZXn-Kt6I",
        "outputId": "c2f605bd-67b6-49aa-dcee-a321cc486520"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(\"I found that the opposite of hate is\")\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q9k44XwKzDu",
        "outputId": "730de742-61f3-4c72-99ea-0afc6dca3e2e"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(\"Yesterday, my dog and I walked to the\")\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "UL95pyiHXm89",
        "outputId": "f7b408a4-d35c-4b24-dd9b-3ce68af4ce47"
      },
      "outputs": [],
      "source": [
        "model = model.to('cpu')\n",
        "tokenizer.decode(model.generate(**tokenizer(\"I found that another word for like is\", return_tensors='pt'), do_sample=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MrkF41xYrlH"
      },
      "outputs": [],
      "source": [
        "new_embeddings =\n",
        "embeddings[-3:,:] = new_embeddings\n",
        "params['transformer.wte.weight'][-3:,:] = new_embeddings\n",
        "model.load_state_dict(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USnsO-xLHdtx"
      },
      "outputs": [],
      "source": [
        "def find_similar_words(word, top_k=15):\n",
        "    word_embedding = model.transformer.wte.weight[tokenizer.encode(word)[0]].detach().cpu().numpy()\n",
        "\n",
        "    # Compute cosine similarity between word embedding and all other embeddings\n",
        "    embeddings = model.transformer.wte.weight.detach().cpu().numpy()\n",
        "    similarities = cosine_similarity([word_embedding], embeddings)[0]\n",
        "\n",
        "    # Get indices of top-k similar words\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # Decode and return top-k similar words\n",
        "    similar_words = [tokenizer.decode([index]) for index in top_indices]\n",
        "    return similar_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54o4j0a_HfpT",
        "outputId": "b64a5ad4-96f9-4d00-e85e-4c3f34aecd3c"
      },
      "outputs": [],
      "source": [
        "# Find similar words based on embeddings\n",
        "\n",
        "for token in new_tokens:\n",
        "  similar_words = find_similar_words(token)\n",
        "  print(\"\\nSimilar words to\", token + \":\")\n",
        "  for similar_word in similar_words:\n",
        "      print(similar_word)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
