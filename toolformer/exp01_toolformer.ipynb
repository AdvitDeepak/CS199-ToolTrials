{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toolformer Experiment - Lack of Usability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "oex7VXDqSpLD",
        "outputId": "6741236b-306a-4411-88c0-f1d615e05a8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 131/131 [00:01<00:00, 67.37it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-175b5d555bc4>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# (5) fine-tune on the filtered results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mfiltered_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# then, once you see the 'finetune complete' message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<@beartype(toolformer_pytorch.toolformer_pytorch.Toolformer.forward) at 0x7f3c619a43a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_getrandbits, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/toolformer_pytorch/toolformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, return_after_generating_api_calls, return_after_making_api_calls, return_after_filtering_api_calls, return_after_filtering_by_api_response)\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_data_with_api_calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_data_with_api_calls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'your model failed to follow instructions and make API calls. please try a better model or do some better prompt engineering'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mdata_with_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_api_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_data_with_api_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: your model failed to follow instructions and make API calls. please try a better model or do some better prompt engineering"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from toolformer_pytorch import Toolformer, PaLM\n",
        "\n",
        "# simple calendar api call - function that returns a string\n",
        "\n",
        "def Calendar():\n",
        "    import datetime\n",
        "    from calendar import day_name, month_name\n",
        "    now = datetime.datetime.now()\n",
        "    return f'Today is {day_name[now.weekday()]}, {month_name[now.month]} {now.day}, {now.year}.'\n",
        "\n",
        "# prompt for teaching it to use the Calendar function from above\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to add calls to a Calendar API to a piece of text.\n",
        "The API calls should help you get information required to complete the text.\n",
        "You can call the API by writing \"[Calendar()]\"\n",
        "Here are some examples of API calls:\n",
        "Input: Today is the first Friday of the year.\n",
        "Output: Today is the first [Calendar()] Friday of the year.\n",
        "Input: The president of the United States is Joe Biden.\n",
        "Output: The president of the United States is [Calendar()] Joe Biden.\n",
        "Input: [input]\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "data = [\n",
        "    \"The store is never open on the weekend, so today it is closed.\",\n",
        "    \"The number of days from now until Christmas is 30\",\n",
        "    \"The current day of the week is Wednesday.\"\n",
        "]\n",
        "\n",
        "# model - here using PaLM, but any nn.Module that returns logits in the shape (batch, seq, num_tokens) is fine\n",
        "\n",
        "model = PaLM(\n",
        "    dim = 512,\n",
        "    depth = 2,\n",
        "    heads = 8,\n",
        "    dim_head = 64\n",
        ").cuda()\n",
        "\n",
        "# toolformer\n",
        "\n",
        "toolformer = Toolformer(\n",
        "    model = model,\n",
        "    model_seq_len = 256,\n",
        "    teach_tool_prompt = prompt,\n",
        "    tool_id = 'Calendar',\n",
        "    tool = Calendar,\n",
        "    finetune = True\n",
        ")\n",
        "\n",
        "# invoking this will\n",
        "# (1) prompt the model with your inputs (data), inserted into [input] tag\n",
        "# (2) with the sampled outputs, filter out the ones that made proper API calls\n",
        "# (3) execute the API calls with the `tool` given\n",
        "# (4) filter with the specialized filter function (which can be used independently as shown in the next section)\n",
        "# (5) fine-tune on the filtered results\n",
        "\n",
        "filtered_stats = toolformer(data)\n",
        "\n",
        "# then, once you see the 'finetune complete' message\n",
        "\n",
        "response = toolformer.sample_model_with_api_calls(\"How many days until the next new years?\")\n",
        "\n",
        "# hopefully you see it invoke the calendar and utilize the response of the api call..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BWj9z7MVTCZS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from toolformer_pytorch import (\n",
        "    Toolformer,\n",
        "    PaLM,\n",
        "    filter_tokens_with_api_response\n",
        ")\n",
        "\n",
        "# model\n",
        "\n",
        "palm = PaLM(\n",
        "    dim = 512,\n",
        "    num_tokens = 20000,\n",
        "    depth = 2,\n",
        "    heads = 8,\n",
        "    dim_head = 64\n",
        ").cuda()\n",
        "\n",
        "# mock some tokens\n",
        "\n",
        "mock_start_pos = 512\n",
        "mock_api_call_length = 10\n",
        "mock_api_start_id = 19998\n",
        "mock_api_stop_id = 19999\n",
        "\n",
        "tokens = torch.randint(0, 20000, (10, 1024)).cuda()\n",
        "tokens_with_api_response = torch.randint(0, 20000, (10, 1024)).cuda()\n",
        "tokens_without_api_response = torch.randint(0, 20000, (10, 1024)).cuda()\n",
        "\n",
        "tokens_with_api_response[:, mock_start_pos] = mock_api_start_id\n",
        "tokens_with_api_response[:, mock_start_pos + mock_api_call_length] = mock_api_stop_id\n",
        "\n",
        "tokens_without_api_response[:, mock_start_pos] = mock_api_start_id\n",
        "tokens_without_api_response[:, mock_start_pos + mock_api_call_length] = mock_api_stop_id\n",
        "\n",
        "# filter\n",
        "\n",
        "filtered_results = filter_tokens_with_api_response(\n",
        "    model = palm,\n",
        "    tokens = tokens,\n",
        "    tokens_with_api_response = tokens_with_api_response,\n",
        "    tokens_without_api_response = tokens_without_api_response,\n",
        "    filter_threshold = 1.,\n",
        "    api_start_token_id = mock_api_start_id,\n",
        "    api_end_token_id = mock_api_stop_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrGGKWy5TE-l",
        "outputId": "222effb7-1935-462a-a463-f0c850b7ac25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FilteredResults(num_passed=0, num_failed=10, selected_indices=tensor([], device='cuda:0', dtype=torch.int64), selected_mask=tensor([False, False, False, False, False, False, False, False, False, False],\n",
            "       device='cuda:0'), filtered_tokens=tensor([], device='cuda:0', size=(0, 1024), dtype=torch.int64), filtered_tokens_without_api_response=tensor([], device='cuda:0', size=(0, 1024), dtype=torch.int64), filtered_tokens_with_api_response=tensor([], device='cuda:0', size=(0, 1024), dtype=torch.int64))\n"
          ]
        }
      ],
      "source": [
        "print(filtered_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
